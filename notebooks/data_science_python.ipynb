{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Data Science in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
    "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
    "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
    "  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n",
    "\n",
    "  ga('create', 'UA-44029069-1', 'auto');\n",
    "  ga('send', 'pageview');\n",
    "\n",
    "</script>\n",
    "\n",
    "This notebook accompanies my talk on \"Data Science with Python\" at the [University of Economics](https://www.vse.cz/english/) in Prague, December 2014. Questions & comments welcome [@RadimRehurek](https://twitter.com/radimrehurek).\n",
    "\n",
    "The goal of this talk is to demonstrate some high level, introductory concepts behind (text) machine learning. The concepts are demonstrated by concrete code examples in this notebook, which you can run yourself (after installing IPython, see below), on your own computer.\n",
    "\n",
    "The talk audience is expected to have some basic programming knowledge (though not necessarily Python) and some basic introductory data mining background. This is *not* an \"advanced talk\" for machine learning experts.\n",
    "\n",
    "The code examples build a working, executable prototype: an app to classify phone SMS messages in English (well, the \"SMS kind\" of English...) as either \"spam\" or \"ham\" (=not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://radimrehurek.com/data_science_python/python.png)](http://xkcd.com/353/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language used throughout will be [Python](https://www.python.org/), a general purpose language helpful in all parts of the pipeline: I/O, data wrangling and preprocessing, model training and evaluation. While Python is by no means the only choice, it offers a unique combination of flexibility, ease of development and performance, thanks to its mature scientific computing ecosystem. Its vast, open source ecosystem also avoids the lock-in (and associated bitrot) of any single specific framework or library.\n",
    "\n",
    "Python (and of most its libraries) is also platform independent, so you can run this notebook on Windows, Linux or OS X without a change.\n",
    "\n",
    "One of the Python tools, the IPython notebook = interactive Python rendered as HTML, you're watching right now. We'll go over other practical tools, widely used in the data science industry, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dFb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "**Want to run the examples below interactively? (optional)**\n",
    "  <ol>\n",
    "  <li>Install the (free) [Anaconda](https://store.continuum.io/cshop/anaconda/) Python distribution, including Python itself.</li>\n",
    "  <li>Install the \"natural language processing\" TextBlob library: [instructions here](http://textblob.readthedocs.org/en/dev/install.html).</li>\n",
    "  <li>Download the source for this notebook to your computer: [http://radimrehurek.com/data_science_python/data_science_python.ipynb](http://radimrehurek.com/data_science_python/data_science_python.ipynb) and run it with:<br />\n",
    "  `$ ipython notebook data_science_python.ipynb`</li>\n",
    "  <li>Watch the [IPython tutorial video](https://www.youtube.com/watch?v=H6dLGQw9yFQ) for notebook navigation basics.</li>\n",
    "  <li>Run the first code cell below; if it executes without errors, you're good to go!\n",
    "  </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end example: automated spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data, look around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the *real* first step (fleshing out specs, finding out what is it we want to be doing -- often highly non-trivial in practice!), let's download the dataset we'll be using in this demo. Go to https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection and download the zip file. Unzip it under `data` subdirectory. You should see a file called `SMSSpamCollection`, about 0.5MB in size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ ls -l data\n",
    "total 1352\n",
    "-rw-r--r--@ 1 kofola  staff  477907 Mar 15  2011 SMSSpamCollection\n",
    "-rw-r--r--@ 1 kofola  staff    5868 Apr 18  2011 readme\n",
    "-rw-r-----@ 1 kofola  staff  203415 Dec  1 15:30 smsspamcollection.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains **a collection of more than 5 thousand SMS phone messages** (see the `readme` file for more info):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_FILENAME =('/Users/zzhang/Documents/DataSci_Project/py2ml/Data/smsspamcollection/SMSSpamCollection')\n",
    "\n",
    "messages = [line.rstrip() for line in open(DATA_FILENAME)]\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of texts is also sometimes called \"corpus\". Let's print the first ten messages in this SMS corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:10]):\n",
    "    print(message_no, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is a [TSV](http://en.wikipedia.org/wiki/Tab-separated_values) (\"tab separated values\") file, where the first column is a label saying whether the given message is a normal message (\"ham\") or \"spam\". The second column is the message itself.\n",
    "\n",
    "This corpus will be our labeled training set. Using these ham/spam examples, we'll **train a machine learning model to learn to discriminate between ham/spam automatically**. Then, with a trained model, we'll be able to **classify arbitrary unlabeled messages** as ham or spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://radimrehurek.com/data_science_python/plot_ML_flow_chart_11.png)](http://www.astroml.org/sklearn_tutorial/general_concepts.html#supervised-learning-model-fit-x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6      ham  Even my brother is not like to speak with me. ...\n",
      "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8     spam  WINNER!! As a valued network customer you have...\n",
      "9     spam  Had your mobile 11 months or more? U R entitle...\n",
      "10     ham  I'm gonna be home soon and i don't want to tal...\n",
      "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "12    spam  URGENT! You have won a 1 week FREE membership ...\n",
      "13     ham  I've been searching for the right words to tha...\n",
      "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "15    spam  XXXMobileMovieClub: To use your credit, click ...\n",
      "16     ham                         Oh k...i'm watching here:)\n",
      "17     ham  Eh u remember how 2 spell his name... Yes i di...\n",
      "18     ham  Fine if thats the way u feel. Thats the way ...\n",
      "19    spam  England v Macedonia - dont miss the goals/team...\n",
      "20     ham          Is that seriously how you spell his name?\n",
      "21     ham    I‘m going to try for 2 months ha ha only joking\n",
      "22     ham  So ü pay first lar... Then when is da stock co...\n",
      "23     ham  Aft i finish my lunch then i go str down lor. ...\n",
      "24     ham  Ffffffffff. Alright no way I can meet up with ...\n",
      "25     ham  Just forced myself to eat a slice. I'm really ...\n",
      "26     ham                     Lol your always so convincing.\n",
      "27     ham  Did you catch the bus ? Are you frying an egg ...\n",
      "28     ham  I'm back &amp; we're packing the car now, I'll...\n",
      "29     ham  Ahhh. Work. I vaguely remember that! What does...\n",
      "...    ...                                                ...\n",
      "5544   ham           Armand says get your ass over to epsilon\n",
      "5545   ham             U still havent got urself a jacket ah?\n",
      "5546   ham  I'm taking derek &amp; taylor to walmart, if I...\n",
      "5547   ham      Hi its in durban are you still on this number\n",
      "5548   ham         Ic. There are a lotta childporn cars then.\n",
      "5549  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5550   ham                 No, I was trying it all weekend ;V\n",
      "5551   ham  You know, wot people wear. T shirts, jumpers, ...\n",
      "5552   ham        Cool, what time you think you can get here?\n",
      "5553   ham  Wen did you get so spiritual and deep. That's ...\n",
      "5554   ham  Have a safe trip to Nigeria. Wish you happines...\n",
      "5555   ham                        Hahaha..use your brain dear\n",
      "5556   ham  Well keep in mind I've only got enough gas for...\n",
      "5557   ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
      "5558   ham  Yes i have. So that's why u texted. Pshew...mi...\n",
      "5559   ham  No. I meant the calculation is the same. That ...\n",
      "5560   ham                             Sorry, I'll call later\n",
      "5561   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
      "5562   ham                  Anything lor. Juz both of us lor.\n",
      "5563   ham  Get me out of this dump heap. My mom decided t...\n",
      "5564   ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
      "5565   ham                                Ard 6 like dat lor.\n",
      "5566   ham  Why don't you wait 'til at least wednesday to ...\n",
      "5567   ham                                       Huh y lei...\n",
      "5568  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5570   ham               Will ü b going to esplanade fr home?\n",
      "5571   ham  Pity, * was in mood for that. So...any other s...\n",
      "5572   ham  The guy did some bitching but I acted like i'd...\n",
      "5573   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5574 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "messages = pandas.read_csv(DATA_FILENAME, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `pandas`, we can also view aggregate statistics easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ham</th>\n",
       "      <th>count</th>\n",
       "      <td>4827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">spam</th>\n",
       "      <th>count</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        message\n",
       "label                                                          \n",
       "ham   count                                                4827\n",
       "      unique                                               4518\n",
       "      top                                Sorry, I'll call later\n",
       "      freq                                                   30\n",
       "spam  count                                                 747\n",
       "      unique                                                653\n",
       "      top     Please call our customer service representativ...\n",
       "      freq                                                    4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I often use value_counts.\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long are the messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  length\n",
      "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
      "1   ham                      Ok lar... Joking wif u oni...      29\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
      "3   ham  U dun say so early hor... U c already then say...      49\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...      61\n"
     ]
    }
   ],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))\n",
    "print(messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1107684e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEACAYAAACQx1DIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JTBIklqymCRJDX1/wGHpz6+FftE5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110f79d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5574.000000\n",
       "mean       80.478292\n",
       "std        59.848302\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is that super long message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\"]\n"
     ]
    }
   ],
   "source": [
    "print(list(messages.message[messages.length > 900]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any difference in message length between spam and ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1109ffe80>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x110b11588>], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAESCAYAAADjS5I+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111004710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good fun, but how do we make computer understand the plain text messages themselves? Or can it under such malformed gibberish at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll massage the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "The mapping is not 1-to-1; we'll use the [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model) approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "As a first step, let's write a function that will split a message into its individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_tokens(message):\n",
    "    message = str(message)  # convert bytes into proper unicode\n",
    "    return TextBlob(message).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'message', 'length'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the original texts again:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here are the same messages, tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, until, jurong, point, crazy, Available, o...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3    [U, dun, say, so, early, hor, U, c, already, t...\n",
       "4    [Nah, I, do, n't, think, he, goes, to, usf, he...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.message.head().apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP questions:\n",
    "\n",
    "1. Do capital letters carry information?\n",
    "2. Does distinguishing inflected form (\"goes\" vs. \"go\") carry information?\n",
    "3. Do interjections, determiners carry information?\n",
    "\n",
    "In other words, we want to better \"normalize\" the text.\n",
    "\n",
    "With textblob, we'd detect [part-of-speech (POS)](http://www.ling.upenn.edu/courses/Fall_2007/ling001/penn_treebank_pos.html) tags with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('world', 'NN'),\n",
       " ('how', 'WRB'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('going', 'VBG')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Hello world, how is it going?\").tags  # list of (word, POS) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and normalize words into their base form ([lemmas](http://en.wikipedia.org/wiki/Lemmatisation)) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, do, n't, think, he, go, to, usf, he, ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = str(message).lower()\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "messages.message.head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better. You can probably think of many more ways to improve the preprocessing: decoding HTML entities (those `&amp;` and `&lt;` we saw above); filtering out stop words (pronouns etc); adding more features, such as an word-in-all-caps indicator and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "\n",
    "Doing that requires essentially three steps, in the bag-of-words model:\n",
    "\n",
    "1. counting how many times does a word occur in each message (term frequency)\n",
    "2. weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. normalizing the vectors to unit length, to abstract from the original text length (L2 norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector has as many dimensions as there are unique words in the SMS corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8874\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used `scikit-learn` (`sklearn`), a powerful Python library for teaching machine learning. It contains a multitude of various methods and options.\n",
    "\n",
    "Let's take one text message and get its bag-of-words counts as a vector, putting to use our new `bow_transformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "message5 = messages['message'][5]\n",
    "\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1158)\t1\n",
      "  (0, 1899)\t1\n",
      "  (0, 2897)\t1\n",
      "  (0, 2927)\t1\n",
      "  (0, 4021)\t1\n",
      "  (0, 6736)\t2\n",
      "  (0, 7111)\t1\n",
      "  (0, 7698)\t1\n",
      "  (0, 8013)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 21)\t2\n",
      "  (1, 567)\t1\n",
      "  (1, 1203)\t1\n",
      "  (1, 1466)\t1\n",
      "  (1, 1571)\t1\n",
      "  (1, 2110)\t1\n",
      "  (1, 2539)\t1\n",
      "  (1, 3386)\t1\n",
      "  (1, 3447)\t1\n",
      "  (1, 3504)\t1\n",
      "  (1, 3929)\t1\n",
      "  (1, 4102)\t1\n",
      "  (1, 4315)\t2\n",
      "  (1, 4732)\t1\n",
      "  (1, 5466)\t1\n",
      "  (1, 5533)\t1\n",
      "  (1, 5628)\t1\n",
      "  (1, 6361)\t1\n",
      "  (1, 6821)\t1\n",
      "  (1, 7127)\t1\n",
      "  (1, 7310)\t1\n",
      "  (1, 7332)\t1\n",
      "  (1, 7590)\t1\n",
      "  (1, 7704)\t1\n",
      "  (1, 7812)\t2\n",
      "  (1, 8103)\t1\n",
      "  (1, 8389)\t1\n",
      "  (1, 8555)\t1\n",
      "  (1, 8684)\t1\n",
      "  (1, 8752)\t1\n",
      "  (1, 8806)\t1\n",
      "(2, 8874)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "\n",
    "# print(bow4)\n",
    "# print(bow4.shape)\n",
    "\n",
    "bow4n5 = bow_transformer.transform([message4, message5])\n",
    "\n",
    "print(bow4n5)\n",
    "print(bow4n5.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, nine unique words in message nr. 4, two of them appear twice, the rest only once. Sanity check: what are these words the appear twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[6736])\n",
    "print(bow_transformer.get_feature_names()[8013])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words counts for the entire SMS corpus are a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (5574, 8874)\n",
      "number of non-zeros: 80272\n",
      "sparsity: 0.16%\n"
     ]
    }
   ],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "print('sparse matrix shape:', messages_bow.shape)\n",
    "print('number of non-zeros:', messages_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, after the counting, the term weighting and normalization can be done with [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf), using scikit-learn's `TfidfTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8013)\t0.305114653686\n",
      "  (0, 7698)\t0.225299911221\n",
      "  (0, 7111)\t0.191390347987\n",
      "  (0, 6736)\t0.523371210191\n",
      "  (0, 4021)\t0.456354991921\n",
      "  (0, 2927)\t0.32967579251\n",
      "  (0, 2897)\t0.303693312742\n",
      "  (0, 1899)\t0.24664322833\n",
      "  (0, 1158)\t0.274934159477\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the IDF (inverse document frequency) of the word `\"u\"`? Of word `\"university\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.85068150539\n",
      "8.23975323521\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 8874)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a multitude of ways in which data can be proprocessed and vectorized. These two steps, also called \"feature engineering\", are typically the most time consuming and \"unsexy\" parts of building a predictive pipeline, but they are very important and require some experience. The trick is to evaluate constantly: analyze model for the errors it makes, improve data cleaning & preprocessing, brainstorm for new features, evaluate..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training a model, detecting spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messages represented as vectors, we can finally train our spam/ham classifier. This part is pretty straightforward, and there are many libraries that realize the training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using scikit-learn here, choosing the [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 2.05 ms, total: 19.4 ms\n",
      "Wall time: 19.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time spam_detector = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detector.predict(tfidf4)[0])\n",
    "print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! You can try it with your own texts, too.\n",
    "\n",
    "A natural question is to ask, how many messages do we classify correctly overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detector.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.969501255831\n",
      "confusion matrix\n",
      " [[4827    0]\n",
      " [ 170  577]]\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "print('accuracy', accuracy_score(messages['label'], all_predictions))\n",
    "print('confusion matrix\\n', confusion_matrix(messages['label'], all_predictions))\n",
    "print('(row=expected, col=predicted)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1155b77f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAD0CAYAAABqz8huAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOMystP8PVgEYStk1Yj0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1153acb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(confusion_matrix(messages['label'], all_predictions), cmap=plt.cm.binary, interpolation='nearest')\n",
    "plt.title('confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('expected label')\n",
    "plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      1.00      0.98      4827\n",
      "       spam       1.00      0.77      0.87       747\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few possible metrics for evaluating model performance. Which one is the most suitable depends on the task. For example, the cost of mispredicting \"spam\" as \"ham\" is probably much lower than mispredicting \"ham\" as \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: How to run experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above \"evaluation\", we committed a cardinal sin. For simplicity of demonstration, we evaluated accuracy on the same data we used for training. **Never evaluate on the same dataset you train on! Bad! Incest!**\n",
    "\n",
    "Such evaluation tells us nothing about the true predictive power of our model. If we simply remembered each example during training, the accuracy on training data would trivially be 100%, even though we wouldn't be able to classify any new messages.\n",
    "\n",
    "A proper way is to split the data into a training/test set, where the model only ever sees the **training data** during its model fitting and parameter tuning. The **test data** is never used in any way -- thanks to this process, we make sure we are not \"cheating\", and that our final evaluation on test data is representative of true predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459 1115 5574\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = \\\n",
    "    train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as requested, the test size is 20% of the entire dataset (1115 messages out of total 5574), and the training is the rest (4459 out of 5574)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap the entire pipeline up to this point, putting the steps explicitly into scikit-learn's `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=split_into_lemmas)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common practice is to partition the training set again, into smaller subsets; for example, 5 equally sized subsets. Then we train the model on four parts, and compute accuracy on the last part (called \"validation set\"). Repeated five times (taking different part for evaluation each time), we get a sense of model \"stability\". If the model gives wildly different scores for different subsets, it's a sign something is wrong (bad data, or bad model variance). Go back, analyze errors, re-check input data for garbage, re-check data cleaning.\n",
    "\n",
    "In our case, everything goes smoothly though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94407159  0.96188341  0.9529148   0.9529148   0.9573991   0.93946188\n",
      "  0.95515695  0.94394619  0.95730337  0.94382022]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline,  # steps to convert raw messages into models\n",
    "                         msg_train,  # training data\n",
    "                         label_train,  # training labels\n",
    "                         cv=10,  # split data randomly into 10 parts: 9 for training, 1 for scoring\n",
    "                         scoring='accuracy',  # which scoring metric?\n",
    "                         n_jobs=-1,  # -1 = use all cores = faster\n",
    "                         )\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are indeed a little bit worse than when we trained on the entire dataset (5574 training examples, accuracy 0.97). They are fairly stable though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950887231392 0.00711373323518\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question is, how can we improve this model? The scores are already high here, but how would we go about improving a model in general?\n",
    "\n",
    "Naive Bayes is an example of a [high bias - low variance](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) classifier (aka simple and stable, not prone to overfitting). An example from the opposite side of the spectrum would be Nearest Neighbour (kNN) classifiers, or Decision Trees, with their low bias but high variance (easy to overfit). Bagging (Random Forests) as a way to lower variance, by training many (high-variance) models and averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://radimrehurek.com/data_science_python/plot_bias_variance_examples_2.png)](http://www.astroml.org/sklearn_tutorial/practical.html#bias-variance-over-fitting-and-under-fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "\n",
    "* **high bias** = classifer is opinionated. Not as much room to change its mind with data, it has its own ideas. On the other hand, not as much room it can fool itself into overfitting either (picture on the left).\n",
    "* **low bias** = classifier more obedient, but also more neurotic. Will do exactly what you ask it to do, which, as everybody knows, can be a real nuisance (picture on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time plot_learning_curve(pipeline, \"accuracy vs. training set size\", msg_train, label_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We're effectively training on 64% of all available data: we reserved 20% for the test set above, and the 5-fold cross validation reserves another 20% for validation sets => `0.8*0.8*5574=3567` training examples left.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since performance keeps growing, both for training and cross validation scores, we see our model is not complex/flexible enough to capture all nuance, given little data. In this particular case, it's not very pronounced, since the accuracies are high anyway.\n",
    "\n",
    "At this point, we have two options:\n",
    "\n",
    "1. use more training data, to overcome low model complexity\n",
    "2. use a more complex (lower bias) model to start with, to get more out of the existing data\n",
    "\n",
    "Over the last years, as massive training data collections become more available, and as machines get faster, approach 1. is becoming more and more popular (simpler algorithms, more data). Straightforward algorithms, such as Naive Bayes, also have the added benefit of being easier to interpret (compared to some more complex, black-box models, like neural networks).\n",
    "\n",
    "Knowing how to evaluate models properly, we can now explore how different parameters affect the performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: How to tune parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've seen so far is only a tip of the iceberg: there are many other parameters to tune. One example is what algorithm to use for training.\n",
    "\n",
    "We've used Naive Bayes above, but scikit-learn supports many classifiers out of the box: Support Vector Machines, Nearest Neighbours, Decision Trees, Ensamble methods..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://radimrehurek.com/data_science_python/drop_shadows_background.png)](http://peekaboo-vision.blogspot.cz/2013/01/machine-learning-cheat-sheet-for-scikit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask: What is the effect of IDF weighting on accuracy? Does the extra processing cost of lemmatization (vs. just plain words) really help?\n",
    "\n",
    "Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'bow__analyzer': (split_into_lemmas, split_into_tokens),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(label_train, n_folds=5),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time nb_detector = grid.fit(msg_train, label_train)\n",
    "print nb_detector.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(best parameter combinations are displayed first: in this case, `use_idf=True` and `analyzer=split_into_lemmas` take the prize).\n",
    "\n",
    "A quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nb_detector.predict_proba([\"Hi mom, how are you?\"])[0]\n",
    "print nb_detector.predict_proba([\"WINNER! Credit for free!\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict_proba` returns the predicted probability for each class (ham, spam). In the first case, the message is predicted to be ham with > 99% probability, and spam with < 1%. So if forced to choose, the model will say \"ham\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nb_detector.predict([\"Hi mom, how are you?\"])[0]\n",
    "print nb_detector.predict([\"WINNER! Credit for free!\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And overall scores on the test set, the one we haven't used at all during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = nb_detector.predict(msg_test)\n",
    "print confusion_matrix(label_test, predictions)\n",
    "print classification_report(label_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then the realistic predictive performance we can expect from our spam detection pipeline, when using lowercase with lemmatization, TF-IDF and Naive Bayes for classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with another classifier: [Support Vector Machines (SVM)](http://en.wikipedia.org/wiki/Support_vector_machine). SVMs are a great starting point when classifying text data, getting state of the art results very quickly and with pleasantly little tuning (although a bit more than Naive Bayes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=split_into_lemmas)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC()),  # <== change here\n",
    "])\n",
    "\n",
    "# pipeline parameters to automatically explore and tune\n",
    "param_svm = [\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm,  # pipeline from above\n",
    "    param_grid=param_svm,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(label_train, n_folds=5),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time svm_detector = grid_svm.fit(msg_train, label_train) # find the best combination from param_svm\n",
    "print svm_detector.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So apparently, linear kernel with `C=1` is the best parameter combination.\n",
    "\n",
    "Sanity check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print svm_detector.predict([\"Hi mom, how are you?\"])[0]\n",
    "print svm_detector.predict([\"WINNER! Credit for free!\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusion_matrix(label_test, svm_detector.predict(msg_test))\n",
    "print classification_report(label_test, svm_detector.predict(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then the realistic predictive performance we can expect from our spam detection pipeline, when using SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Productionalizing a predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With basic analysis and tuning done, the real work (engineering) begins.\n",
    "\n",
    "The final step for a production predictor would be training it on the entire dataset again, to make full use of all the data available. We'd use the best parameters found via cross validation above, of course. This is very similar to what we did in the beginning, but this time having insight into its behaviour and stability. Evaluation was done honestly, on distinct train/test subset splits.\n",
    "\n",
    "The final predictor can be serialized to disk, so that the next time we want to use it, we can skip all training and use the trained model directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the spam detector to disk after training\n",
    "with open('sms_spam_detector.pkl', 'wb') as fout:\n",
    "    cPickle.dump(svm_detector, fout)\n",
    "\n",
    "# ...and load it back, whenever needed, possibly on a different machine\n",
    "svm_detector_reloaded = cPickle.load(open('sms_spam_detector.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded result is an object that behaves identically to the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'before:', svm_detector.predict([message4])[0]\n",
    "print 'after:', svm_detector_reloaded.predict([message4])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important part of a production implementation is **performance**. After a rapid, iterative model tuning and parameter search as shown here, a well performing model can be translated into a different language and optimized. Would trading a few accuracy points give us a smaller, faster model? Is it worth optimizing memory usage, perhaps using `mmap` to share memory across processes?\n",
    "\n",
    "Note that optimization is not always necessary; always start with actual profiling.\n",
    "\n",
    "Other things to consider here, for a production pipeline, are **robustness** (service failover, redundancy, load balancing), **monitoring** (incl. auto-alerts on anomalies) and **HR fungibility** (avoiding \"knowledge silos\" of how things are done, arcane/lock-in technologies, black art of tuning results). These days, even the open source world can offer viable solutions in all of these areas. All the tool shown today are free for commercial use, under OSI-approved open source licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other practical concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data sparsity\n",
    "\n",
    "online learning, data streams\n",
    "\n",
    "`mmap` for memory sharing, system \"cold-start\" load times\n",
    "\n",
    "scalability, distributed (cluster) processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data *not* structured. Gaining insight, no intrinsic evaluation possible (or else becomes supervised learning!).\n",
    "\n",
    "How can we train *anything* without labels? What kind of sorcery is this?\n",
    "\n",
    "[Distributional hypothesis](http://en.wikipedia.org/wiki/Distributional_semantics): *\"Words that occur in similar contexts tend to have similar meanings\"*. Context = sentence, document, sliding window...\n",
    "\n",
    "Check out this [live demo of Google's word2vec](http://radimrehurek.com/2014/02/word2vec-tutorial/#app) for unsupervised learning. Simple model, large data (Google News, 100 billion words, no labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A static (non-interactive version) of this notebook rendered into HTML at [http://radimrehurek.com/data_science_python](http://radimrehurek.com/data_science_python) (you're probably watching it right now, but just in case).\n",
    "\n",
    "Interactive notebook source lives on GitHub: [https://github.com/piskvorky/data_science_python](https://github.com/piskvorky/data_science_python) (see top for installation instructions).\n",
    "\n",
    "My company, [RaRe Technologies](http://rare-technologies.com/), lives at the exciting intersection of **pragmatic, commercial system building** and **cutting edge research**. Interested in interning / collaboration? [Get in touch](http://rare-technologies.com/#contactus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
